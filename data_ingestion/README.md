Here‚Äôs a cleaned-up, polished, and fully corrected version of your documentation for `data_ingestion__events.py`. I‚Äôve fixed formatting, clarified instructions, and aligned table/column names with your script.

---

# Data Ingestion ‚Äî `data_ingestion__events.py`

## Pre-requisites

1. **Python Version**
   Python 3.8 or higher.

2. **Required Python Packages**

   ```bash
   pip install pandas pyarrow numpy psycopg2-binary python-dotenv
   ```

   *If using Conda:*

   ```bash
   conda install -c conda-forge pandas pyarrow numpy psycopg2
   ```

3. **PostgreSQL Database**
   Ensure PostgreSQL is running locally or on an accessible host.

4. **Directory Structure**

   ```
   /source_data_folder   # Incoming parquet files
   /archive_folder       # Processed files are moved here
   /logs/                # Ingestion error logs
   ```

5. **Database Tables**

   * `raw.events` ‚Äî stores ingested data
   * `raw.ingestion_log` ‚Äî tracks already processed files

---

## ‚öôÔ∏è Setup

1. **Create a `.env` file** in the root of your project.
   Copy the structure below and update values as needed:

   ```env
   # -------- Database Configuration --------
   DB_NAME=your_database_name
   DB_USER=your_database_user
   DB_PASSWORD=your_database_password
   DB_HOST=localhost
   DB_PORT=5432

   # -------- Directories --------
   SOURCE_DIR=/path/to/your/source/parquet/files
   ARCHIVE_DIR=/path/to/archive/directory

   # -------- Logging --------
   LOG_FILE=/path/to/log/ingestion.log

   # -------- Tables --------
   TARGET_TABLE=database_name.schema_name.table_name
   LOG_TABLE=raw.ingestion_log
   ```

   > The script automatically loads environment variables from `.env` using `python-dotenv`.

---

## ‚öôÔ∏è How It Works

1. Connects to PostgreSQL using credentials from `.env`.
2. Ensures the `raw` schema exists.
3. Creates the ingestion log table (`raw.ingestion_log`) if it doesn‚Äôt exist.
4. Scans the source folder for new `.parquet` files that have not already been ingested.
5. For each new file:

   * Reads the parquet file (using **PyArrow**).
   * Adds metadata columns:

     * `source_file_name` ‚Äî name of the file ingested
     * `ingested_at` ‚Äî timestamp of ingestion
   * Converts `event_data` and `event_context` columns to JSONB-compatible strings.
   * Ensures the target table exists (`raw.events`) with appropriate column types.
   * Inserts all rows into the target table.
   * Logs ingestion in `raw.ingestion_log`.
   * Moves the processed file to the archive folder.

---

## üõ° Error Handling

The script handles:

* **Missing parquet engine** ‚Üí Requires `pyarrow` or `fastparquet`.
* **Invalid JSON** ‚Üí Converts NumPy/Pandas types to JSON-serializable formats; logs failures.
* **Database errors** ‚Üí Rolls back failed transactions and logs details.
* **Duplicate ingestion** ‚Üí Skips already processed files via `raw.ingestion_log`.
* **File movement issues** ‚Üí Ensures processed files are archived even if ingestion partially fails.

---

## ‚úÖ Summary

This script ingests parquet files into PostgreSQL with:

* JSON validation and conversion
* Automatic schema and table creation
* Robust error handling and logging
* Prevention of duplicate ingestion
* Archiving of processed files

---

If you want, I can also **add a step-by-step example of running the script**, showing how a parquet file flows from source ‚Üí database ‚Üí archive, which makes the documentation much more user-friendly. Do you want me to do that?
